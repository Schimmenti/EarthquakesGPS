{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Schimmenti/EarthquakesGPS/blob/main/Dataset.ipynb",
      "authorship_tag": "ABX9TyMwU4M5uC1tOS4wK0Pm3t/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schimmenti/EarthquakesGPS/blob/main/Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Y_QgiMeEx9kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdbfe89-f211-4e80-97aa-e5743a170542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "from scipy.ndimage.filters import maximum_filter1d, minimum_filter1d\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def max_filter1d_valid(a, W, add_heading_nan=True):\n",
        "    hW = (W-1)//2 # Half window size\n",
        "    if(add_heading_nan):\n",
        "      return np.concatenate([np.ones(W-1)*np.nan,maximum_filter1d(a,size=W)[hW:-hW]])\n",
        "    else:\n",
        "      return maximum_filter1d(a,size=W)[hW:-hW]\n",
        "def min_filter1d_valid(a, W, add_heading_nan=True):\n",
        "    hW = (W-1)//2 # Half window size\n",
        "    if(add_heading_nan):\n",
        "      return np.concatenate([np.ones(W-1)*np.nan,minimum_filter1d(a,size=W)[hW:-hW]])\n",
        "    else:\n",
        "      return minimum_filter1d(a,size=W)[hW:-hW]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine(lat1, lat2, delta_long):\n",
        "  return np.sin((lat2-lat1)/2)**2+np.cos(lat1)*np.cos(lat2)*np.sin(delta_long/2)**2"
      ],
      "metadata": {
        "id": "xbH1ADom-e5K"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_lin_regress(x, T, t0, t1, add_heading_nan=True):\n",
        "    m = []\n",
        "    for t in range(t0,t1-T):\n",
        "        m.append(stats.linregress(np.arange(0,T),x[t:t+T])[0])\n",
        "    if(add_heading_nan):\n",
        "      return np.concatenate([np.ones(T)*np.nan, np.array(m)])\n",
        "    else:\n",
        "      return np.array(m)"
      ],
      "metadata": {
        "id": "r5MxN8MCzqln"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_link = \"https://raw.githubusercontent.com/Schimmenti/EarthquakesGPS/main/gps_data/\""
      ],
      "metadata": {
        "id": "CIFz3TRJyjwN"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget \"https://raw.githubusercontent.com/Schimmenti/EarthquakesGPS/main/gps_data/stat_info.pkl\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i45aPIdXylyt",
        "outputId": "e8a8f8f0-0dbf-4112-cfc3-5753c703bee9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-10 12:44:40--  https://raw.githubusercontent.com/Schimmenti/EarthquakesGPS/main/gps_data/stat_info.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17030 (17K) [application/octet-stream]\n",
            "Saving to: ‘stat_info.pkl.2’\n",
            "\n",
            "stat_info.pkl.2     100%[===================>]  16.63K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-10 12:44:40 (11.2 MB/s) - ‘stat_info.pkl.2’ saved [17030/17030]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"stat_info.pkl\",\"rb\") as handle:\n",
        "  station_names, station_pos = pkl.load(handle)"
      ],
      "metadata": {
        "id": "Ee6C9Q0xyp_s"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "LQdZpEyTapI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stat_data = {}\n",
        "for name in station_names:\n",
        "  try:\n",
        "    stat_data[name] = pd.read_csv(base_link + name + \".csv\")\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "o3PW3xswy3E-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in stat_data.keys():\n",
        "  #decimal_years  = stat_data[name][\"DateD\"].values\n",
        "  #years = np.trunc(decimal_years).astype('int')\n",
        "  #days_in_year = 365*np.ones(len(years), dtype=int)\n",
        "  #days_in_year[days_in_year%4==0] += 1\n",
        "  #days = np.round((decimal_years-years)*days_in_year).astype('int')\n",
        "  #starting_date = datetime.date(years[0], 1, 1)\n",
        "  #delta_days = datetime.timedelta(int(days[0]) - 1)\n",
        "  #starting_date += delta_days\n",
        "#\n",
        "  #ending_date = datetime.date(years[-1]-1, 12, 31)\n",
        "  #delta_days = datetime.timedelta(int(days[-1]))\n",
        "  #ending_date += delta_days\n",
        "  #print(ending_date)\n",
        "  #del stat_data[name][\"Date\"]\n",
        "\n",
        "  stat_data[name]['Date'] = pd.to_datetime(stat_data[name]['DateI'], format='%Y%m%d')\n",
        "  stat_data[name].set_index(\"Date\", drop=True, inplace=True)\n",
        "  idx = pd.date_range(stat_data[name].index[0], stat_data[name].index[-1])\n",
        "  stat_data[name] = stat_data[name].reindex(idx, fill_value=np.NaN)"
      ],
      "metadata": {
        "id": "Uf8lbUsfzgyE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"stat_data.pkl\", \"wb\") as handle:\n",
        "  pkl.dump(stat_data,handle)"
      ],
      "metadata": {
        "id": "yZ8vXeUlY5yl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W=9\n",
        "coefs = {}\n",
        "for name in stat_data.keys():\n",
        "  nord = stat_data[name][\"N\"].values\n",
        "  T_w_nord = moving_lin_regress(nord, W, 0, len(nord))\n",
        "  east = stat_data[name][\"E\"].values\n",
        "  T_w_east = moving_lin_regress(east, W, 0, len(east))\n",
        "  up = stat_data[name][\"U\"].values\n",
        "  T_w_up = moving_lin_regress(up, W, 0, len(up))\n",
        "  coefs[name] = np.array([T_w_nord, T_w_east, T_w_up])"
      ],
      "metadata": {
        "id": "paPxBu_2z78a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"T_W=%i.pkl\"%W, \"wb\") as handle:\n",
        "  pkl.dump(coefs,handle)"
      ],
      "metadata": {
        "id": "9P5rDJEggbWE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading"
      ],
      "metadata": {
        "id": "bbFPAmiNZnuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catalog = pd.read_csv(\"https://raw.githubusercontent.com/Schimmenti/EarthquakesGPS/main/catalog.csv\",sep=r\"\\s+\", index_col=\"Date\")"
      ],
      "metadata": {
        "id": "_kcPT7QpaT8h"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W=9\n",
        "with open(\"drive/MyDrive/Colab Notebooks/T_W=%i.pkl\"%W,\"rb\") as handle:\n",
        "  coefs = pkl.load(handle)"
      ],
      "metadata": {
        "id": "g6hZZ__0ZpcN"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/Colab Notebooks/stat_data.pkl\",\"rb\") as handle:\n",
        "  stat_data = pkl.load(handle)"
      ],
      "metadata": {
        "id": "pC97AZBGamy7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excursion"
      ],
      "metadata": {
        "id": "nKEmLHuMcYYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "up_scaling_factor = 4\n",
        "excoefs = {}\n",
        "for key in coefs.keys():\n",
        "  temp = []\n",
        "  for el in coefs[key]:\n",
        "    temp.append(max_filter1d_valid(el, W)-min_filter1d_valid(el, W))\n",
        "  #temp.append(np.sqrt(temp[0]**2+temp[1]**2+(temp[2]/up_scaling_factor)**2))\n",
        "  temp = np.array(temp)\n",
        "  excoefs[key] = temp"
      ],
      "metadata": {
        "id": "BxQDKqYDzmSJ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ex_T_W=%i.pkl\" %W, \"wb\") as handle:\n",
        "  pkl.dump(excoefs,handle)"
      ],
      "metadata": {
        "id": "7B9wAY2S3JJh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catalog"
      ],
      "metadata": {
        "id": "xevVaB4BH2ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catalog = pd.read_csv(\"https://raw.githubusercontent.com/Schimmenti/EarthquakesGPS/main/hauksson_relocated.dat\",sep=r\"\\s+\",header=None)\n",
        "catalog.columns = [\"sec\",\"m\",\"lat\",\"long\",\"dep\"]"
      ],
      "metadata": {
        "id": "JNn3tbnpsmjH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "landers = catalog[catalog[\"m\"]==7.3]\n",
        "landers_date = datetime.date(1992,6,28)\n",
        "landers_time = datetime.timedelta(hours=11, minutes=57, seconds=33)\n",
        "delta_time = datetime.timedelta(seconds=float(landers['sec'].values))\n",
        "catalog_beginning = landers_date+landers_time-delta_time\n",
        "catalog_seconds = catalog['sec'].values.astype('timedelta64[s]')\n",
        "start_date = np.datetime64(catalog_beginning)\n",
        "catalog_dates = start_date + catalog_seconds\n",
        "year_integer =  catalog_dates.astype('datetime64[Y]').astype('int')+1970\n",
        "month_integer =  catalog_dates.astype('datetime64[M]').astype('int')%12+1\n",
        "day_integer = (catalog_dates- catalog_dates.astype('datetime64[M]') + 1).astype('timedelta64[D]').astype('int')+1\n",
        "date_integer =(year_integer*10000+month_integer*100+day_integer)\n",
        "pandas_datetime = pd.to_datetime(date_integer, format='%Y%m%d')\n",
        "catalog[\"Date\"] = pandas_datetime\n",
        "catalog.set_index(\"Date\", drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "xgUekX5HHz__"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catalog.to_csv(\"catalog.csv\", sep=\"\\t\")"
      ],
      "metadata": {
        "id": "GVYhCAwm9rLa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Station position"
      ],
      "metadata": {
        "id": "cxLmoF8m-I3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_array = np.array([station_pos[key] for key in stat_data.keys()])"
      ],
      "metadata": {
        "id": "-SD_ILQ--LG3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lat1 = catalog['lat'].values.reshape(-1,1)\n",
        "lat2 = position_array[:,0].reshape(1,-1)\n",
        "delta_long = catalog['long'].values.reshape(-1,1)-position_array[:,1].reshape(1,-1)"
      ],
      "metadata": {
        "id": "4kR__7C9-j1C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = haversine(lat1,lat2,delta_long)"
      ],
      "metadata": {
        "id": "jiiY8sm8-8Jk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_station_idx = np.argsort(distances, axis=1)"
      ],
      "metadata": {
        "id": "7uB5_MBf_JZn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_dist = 1e-3\n",
        "max_number = 20"
      ],
      "metadata": {
        "id": "mb6R1o6d4rJ6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_day = catalog[\"m\"].groupby(\"Date\").apply(lambda x : (2/3)*np.log10(np.sum(10**(1.5*x))))\n",
        "true_days = m_day.index\n",
        "m_day = m_day.values\n",
        "lat_day = catalog[\"lat\"].groupby(\"Date\").mean().values\n",
        "long_day = catalog[\"long\"].groupby(\"Date\").mean().values\n",
        "dep_day = catalog[\"dep\"].groupby(\"Date\").mean().values"
      ],
      "metadata": {
        "id": "DFI4etj1POk6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catalog_day = pd.DataFrame(columns = [\"m\", \"lat\", \"long\", \"dep\" ])"
      ],
      "metadata": {
        "id": "Gm6hoZFi-tWt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catalog_day[\"m\"] = m_day\n",
        "catalog_day[\"lat\"] = lat_day\n",
        "catalog_day[\"long\"] = long_day\n",
        "catalog_day[\"dep\"] = dep_day\n",
        "catalog_day.index = true_days"
      ],
      "metadata": {
        "id": "3fyQXz1b-6CH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catalog_day.to_csv(\"catalog_day.csv\", sep=\"\\t\")"
      ],
      "metadata": {
        "id": "rvC8A6mW_YJv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qUVzorG5_iHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}